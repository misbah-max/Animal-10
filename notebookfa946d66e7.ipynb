{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2deee71a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-30T17:04:09.186966Z",
     "iopub.status.busy": "2026-01-30T17:04:09.186580Z",
     "iopub.status.idle": "2026-01-30T17:04:27.678006Z",
     "shell.execute_reply": "2026-01-30T17:04:27.677045Z"
    },
    "papermill": {
     "duration": 18.497192,
     "end_time": "2026-01-30T17:04:27.679854",
     "exception": false,
     "start_time": "2026-01-30T17:04:09.182662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU available: True\n",
      "GPU: Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "# T# ====================================================\n",
    "# VETERINARY ANIMAL CLASSIFICATION WITH TRANSFORMERS\n",
    "# Dataset: Animals-10 (Kaggle)\n",
    "# Model: Vision Transformer (ViT) - No Pretrained Weights\n",
    "# Accuracy Target: >85%\n",
    "# Domain: Veterinary (Animal Health & Species Identification)\n",
    "# SDG Alignment: Life on Land (SDG 15)\n",
    "# ====================================================\n",
    "\n",
    "# Install required packages\n",
    "!pip install torch torchvision matplotlib seaborn scikit-learn tqdm pillow pandas -q\n",
    "\n",
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import transforms, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "   \n",
    "            \n",
    "       \n",
    "\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ff2c7f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T17:04:27.685034Z",
     "iopub.status.busy": "2026-01-30T17:04:27.684233Z",
     "iopub.status.idle": "2026-01-30T17:10:54.273907Z",
     "shell.execute_reply": "2026-01-30T17:10:54.272969Z"
    },
    "papermill": {
     "duration": 386.595555,
     "end_time": "2026-01-30T17:10:54.277017",
     "exception": false,
     "start_time": "2026-01-30T17:04:27.681462",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: Tesla P100-PCIE-16GB\n",
      "==================================================\n",
      "LOADING ANIMALS-10 DATASET\n",
      "==================================================\n",
      "‚úì Found dataset at: /kaggle/input/animals10\n",
      "\n",
      "Loading from: /kaggle/input/animals10\n",
      "\n",
      "Exploring directory structure...\n",
      "Found 2 items in directory\n",
      "  translate.py [file]\n",
      "  raw-img/ [directory] - sample: ['cavallo', 'pecora', 'elefante']\n",
      "\n",
      "Loading images...\n",
      "‚úì Loaded 26179 images\n",
      "‚úì Classes: 1\n",
      "‚úì Class names: ['raw-img']\n",
      "\n",
      "Class distribution:\n",
      "  raw-img: 26179 images\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# VETERINARY ANIMAL CLASSIFICATION - FIXED VERSION\n",
    "# Dataset: Animals-10 (Kaggle)\n",
    "# Model: Custom Vision Transformer (No Pretrained Weights)\n",
    "# ====================================================\n",
    "\n",
    "# Install packages\n",
    "!pip install torch torchvision matplotlib seaborn scikit-learn tqdm pillow pandas -q\n",
    "\n",
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import transforms, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# ====================================================\n",
    "# 1. LOAD DATASET (FIXED - NO SHELL COMMANDS IN PYTHON INDENTATION)\n",
    "# ====================================================\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"LOADING ANIMALS-10 DATASET\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Dataset paths to try\n",
    "dataset_paths = [\n",
    "    \"/kaggle/input/animals10/animals10\",\n",
    "    \"/kaggle/input/animals10\",\n",
    "    \"/kaggle/input/animals-10\",\n",
    "    \"/kaggle/input/animal10\"\n",
    "]\n",
    "\n",
    "data_dir = None\n",
    "for path in dataset_paths:\n",
    "    if os.path.exists(path):\n",
    "        data_dir = path\n",
    "        print(f\"‚úì Found dataset at: {path}\")\n",
    "        break\n",
    "\n",
    "# If not found, show instructions\n",
    "if data_dir is None:\n",
    "    print(\"\\n‚ùå Dataset not found.\")\n",
    "    print(\"\\nüìã PLEASE ADD THIS DATASET:\")\n",
    "    print(\"1. Click '+ Add Data' button on Kaggle\")\n",
    "    print(\"2. Search for 'animals10'\")\n",
    "    print(\"3. Use: https://www.kaggle.com/datasets/alessiocorrado99/animals10\")\n",
    "    print(\"\\nUsing synthetic data for demo...\")\n",
    "    # Create demo data\n",
    "    from torchvision.datasets import FakeData\n",
    "    full_dataset = FakeData(size=2000, image_size=(3, 224, 224), num_classes=10, \n",
    "                           transform=transforms.ToTensor())\n",
    "else:\n",
    "    # Load real dataset\n",
    "    print(f\"\\nLoading from: {data_dir}\")\n",
    "    \n",
    "    # Check what's in the directory (using Python, not shell commands)\n",
    "    print(\"\\nExploring directory structure...\")\n",
    "    try:\n",
    "        items = os.listdir(data_dir)\n",
    "        print(f\"Found {len(items)} items in directory\")\n",
    "        \n",
    "        # Show first 10 items\n",
    "        for i, item in enumerate(items[:10]):\n",
    "            item_path = os.path.join(data_dir, item)\n",
    "            if os.path.isdir(item_path):\n",
    "                sub_items = os.listdir(item_path)[:3]\n",
    "                print(f\"  {item}/ [directory] - sample: {sub_items}\")\n",
    "            else:\n",
    "                print(f\"  {item} [file]\")\n",
    "        \n",
    "        # Load with ImageFolder\n",
    "        print(\"\\nLoading images...\")\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.RandomCrop(224),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomRotation(degrees=15),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        full_dataset = datasets.ImageFolder(root=data_dir, transform=train_transform)\n",
    "        print(f\"‚úì Loaded {len(full_dataset)} images\")\n",
    "        print(f\"‚úì Classes: {len(full_dataset.classes)}\")\n",
    "        print(f\"‚úì Class names: {full_dataset.classes}\")\n",
    "        \n",
    "        # Show class distribution\n",
    "        print(\"\\nClass distribution:\")\n",
    "        class_counts = {}\n",
    "        for _, label in full_dataset.samples:\n",
    "            class_name = full_dataset.classes[label]\n",
    "            class_counts[class_name] = class_counts.get(class_name, 0) + 1\n",
    "        \n",
    "        for cls, count in class_counts.items():\n",
    "            print(f\"  {cls}: {count} images\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        print(\"Using synthetic data instead...\")\n",
    "        from torchvision.datasets import FakeData\n",
    "        full_dataset = FakeData(size=2000, image_size=(3, 224, 224), num_classes=10, \n",
    "                               transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ffdd690",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T17:10:54.282224Z",
     "iopub.status.busy": "2026-01-30T17:10:54.281714Z",
     "iopub.status.idle": "2026-01-30T17:10:54.298125Z",
     "shell.execute_reply": "2026-01-30T17:10:54.297302Z"
    },
    "papermill": {
     "duration": 0.021085,
     "end_time": "2026-01-30T17:10:54.299697",
     "exception": false,
     "start_time": "2026-01-30T17:10:54.278612",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PREPARING DATA\n",
      "==================================================\n",
      "Dataset split:\n",
      "  Train: 18325 images\n",
      "  Val: 3926 images\n",
      "  Test: 3928 images\n",
      "\n",
      "‚úì Data loaders created (batch size: 32)\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# 2. DATA TRANSFORMS AND SPLITTING\n",
    "# ====================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PREPARING DATA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "     transforms.RandomRotation(degrees=15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "# Split dataset\n",
    "train_size = int(0.7 * len(full_dataset))\n",
    "val_size = int(0.15 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    full_dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "print(f\"Dataset split:\")\n",
    "print(f\"  Train: {len(train_dataset)} images\")\n",
    "print(f\"  Val: {len(val_dataset)} images\")\n",
    "print(f\"  Test: {len(test_dataset)} images\")\n",
    "# Apply transforms to val/test\n",
    "class TransformedDataset(Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img, label = self.subset[index]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "        val_dataset = TransformedDataset(val_dataset, val_transform)\n",
    "test_dataset = TransformedDataset(test_dataset, val_transform)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"\\n‚úì Data loaders created (batch size: {batch_size})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da5b858d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T17:10:54.304512Z",
     "iopub.status.busy": "2026-01-30T17:10:54.304180Z",
     "iopub.status.idle": "2026-01-30T17:10:55.075211Z",
     "shell.execute_reply": "2026-01-30T17:10:55.074330Z"
    },
    "papermill": {
     "duration": 0.775516,
     "end_time": "2026-01-30T17:10:55.076996",
     "exception": false,
     "start_time": "2026-01-30T17:10:54.301480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "BUILDING MODEL\n",
      "==================================================\n",
      "Model created with 1 classes\n",
      "Total parameters: 51,800,225\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# 3. CUSTOM CNN MODEL - FIXED VERSION\n",
    "# ====================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BUILDING MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AnimalClassifierCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10, input_size=224):\n",
    "        super(AnimalClassifierCNN, self).__init__()\n",
    "        \n",
    "        # -----------------\n",
    "        # Feature extractor\n",
    "        # -----------------\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.25),\n",
    "            \n",
    "            # Block 2\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.25),\n",
    "            \n",
    "            # Block 3\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.25),\n",
    "        )\n",
    "\n",
    "        # -----------------\n",
    "        # Dynamic feature size calculation\n",
    "        # -----------------\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 3, input_size, input_size)\n",
    "            dummy_out = self.features(dummy)\n",
    "            self.flatten_dim = dummy_out.view(1, -1).shape[1]\n",
    "\n",
    "        # -----------------\n",
    "        # Classifier\n",
    "        # -----------------\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.flatten_dim, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)   # Flatten\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# -----------------\n",
    "# Create model\n",
    "# -----------------\n",
    "num_classes = 10\n",
    "if hasattr(full_dataset, 'classes'):\n",
    "    num_classes = len(full_dataset.classes)\n",
    "\n",
    "model = AnimalClassifierCNN(num_classes=num_classes, input_size=224)  # change 224 if needed\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model created with {num_classes} classes\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 59760,
     "sourceId": 840806,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 411.400115,
   "end_time": "2026-01-30T17:10:57.710995",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-30T17:04:06.310880",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
